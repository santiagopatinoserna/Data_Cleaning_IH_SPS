{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧹 Masterclass de Limpieza de Datos con Pandas - Parte 3\n",
    "\n",
    "## Contenido\n",
    "1. [Técnicas Avanzadas de Imputación](#1)\n",
    "2. [Normalización y Escalado](#2)\n",
    "3. [Caso Práctico: Análisis de Ventas](#3)\n",
    "4. [Automatización de la Limpieza](#4)\n",
    "\n",
    "En esta tercera parte exploraremos técnicas más sofisticadas de limpieza de datos y veremos un caso práctico completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuraciones\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Técnicas Avanzadas de Imputación\n",
    "\n",
    "Vamos a explorar diferentes métodos de imputación para datos faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Crear dataset con valores faltantes\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generar datos correlacionados\n",
    "x = np.random.normal(0, 1, n)\n",
    "y = 2 * x + np.random.normal(0, 0.5, n)\n",
    "z = x - y + np.random.normal(0, 0.3, n)\n",
    "\n",
    "df_missing = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y,\n",
    "    'z': z\n",
    "})\n",
    "\n",
    "# Introducir valores faltantes aleatoriamente\n",
    "for col in df_missing.columns:\n",
    "    mask = np.random.random(n) < 0.2  # 20% de valores faltantes\n",
    "    df_missing.loc[mask, col] = np.nan\n",
    "\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "# Visualizar patrones de valores faltantes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_missing.isnull(), cmap='viridis', yticklabels=False)\n",
    "plt.title('Patrón de Valores Faltantes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Imputación Simple\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_imputed_mean = pd.DataFrame(\n",
    "    imputer_mean.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "# 2. Imputación KNN\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df_imputed_knn = pd.DataFrame(\n",
    "    imputer_knn.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "# 3. Imputación por interpolación\n",
    "df_imputed_interp = df_missing.interpolate(method='cubic')\n",
    "\n",
    "# Comparar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Comparación de Métodos de Imputación')\n",
    "\n",
    "# Datos originales con valores faltantes\n",
    "sns.scatterplot(data=df_missing, x='x', y='y', ax=axes[0,0])\n",
    "axes[0,0].set_title('Datos Originales con Valores Faltantes')\n",
    "\n",
    "# Imputación por media\n",
    "sns.scatterplot(data=df_imputed_mean, x='x', y='y', ax=axes[0,1])\n",
    "axes[0,1].set_title('Imputación por Media')\n",
    "\n",
    "# Imputación KNN\n",
    "sns.scatterplot(data=df_imputed_knn, x='x', y='y', ax=axes[1,0])\n",
    "axes[1,0].set_title('Imputación KNN')\n",
    "\n",
    "# Imputación por interpolación\n",
    "sns.scatterplot(data=df_imputed_interp, x='x', y='y', ax=axes[1,1])\n",
    "axes[1,1].set_title('Imputación por Interpolación')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Normalización y Escalado\n",
    "\n",
    "Vamos a explorar diferentes técnicas de normalización y escalado de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Crear datos con diferentes escalas\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "df_scale = pd.DataFrame({\n",
    "    'pequeña_escala': np.random.normal(0, 1, n),\n",
    "    'gran_escala': np.random.normal(1000, 100, n),\n",
    "    'con_outliers': np.concatenate([\n",
    "        np.random.normal(10, 2, n-10),\n",
    "        np.random.normal(50, 5, 10)  # outliers\n",
    "    ])\n",
    "})\n",
    "\n",
    "# Aplicar diferentes escaladores\n",
    "scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'robust': RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_dfs = {}\n",
    "for name, scaler in scalers.items():\n",
    "    scaled_dfs[name] = pd.DataFrame(\n",
    "        scaler.fit_transform(df_scale),\n",
    "        columns=df_scale.columns\n",
    "    )\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Comparación de Métodos de Escalado')\n",
    "\n",
    "# Datos originales\n",
    "df_scale.boxplot(ax=axes[0,0])\n",
    "axes[0,0].set_title('Datos Originales')\n",
    "\n",
    "# Standard Scaler\n",
    "scaled_dfs['standard'].boxplot(ax=axes[0,1])\n",
    "axes[0,1].set_title('Standard Scaler')\n",
    "\n",
    "# MinMax Scaler\n",
    "scaled_dfs['minmax'].boxplot(ax=axes[1,0])\n",
    "axes[1,0].set_title('MinMax Scaler')\n",
    "\n",
    "# Robust Scaler\n",
    "scaled_dfs['robust'].boxplot(ax=axes[1,1])\n",
    "axes[1,1].set_title('Robust Scaler')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Caso Práctico: Análisis de Ventas\n",
    "\n",
    "Vamos a crear un caso práctico completo de limpieza de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Crear dataset de ventas con problemas típicos\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generar fechas\n",
    "fechas_base = pd.date_range(start='2022-01-01', end='2023-12-31', periods=n)\n",
    "fechas = [f.strftime(random.choice(['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%y'])) for f in fechas_base]\n",
    "\n",
    "# Generar productos y categorías\n",
    "productos = ['Laptop', 'Smartphone', 'Tablet', 'Smartwatch', 'Auriculares']\n",
    "categorias = ['Electrónica', 'electronica', 'ELECTRONICA', 'Electrnica']\n",
    "\n",
    "# Crear DataFrame\n",
    "df_ventas = pd.DataFrame({\n",
    "    'fecha': fechas,\n",
    "    'producto': np.random.choice(productos, n),\n",
    "    'categoria': np.random.choice(categorias, n),\n",
    "    'precio': np.random.uniform(100, 1000, n),\n",
    "    'cantidad': np.random.randint(1, 10, n),\n",
    "    'id_cliente': [f'CLI_{i:04d}' for i in range(n)],\n",
    "    'email': [f'cliente{i}@ejemplo.com' if random.random() > 0.1 else 'invalido@.com' for i in range(n)]\n",
    "})\n",
    "\n",
    "# Introducir problemas típicos\n",
    "# 1. Valores nulos\n",
    "df_ventas.loc[np.random.choice(n, 50), 'precio'] = None\n",
    "df_ventas.loc[np.random.choice(n, 30), 'email'] = None\n",
    "\n",
    "# 2. Outliers en precios\n",
    "df_ventas.loc[np.random.choice(n, 10), 'precio'] = np.random.uniform(5000, 10000, 10)\n",
    "\n",
    "# 3. Duplicados\n",
    "df_ventas = pd.concat([df_ventas, df_ventas.sample(n=50)])\n",
    "\n",
    "print(\"Estado inicial del dataset:\")\n",
    "print(df_ventas.info())\n",
    "print(\"\\nValores nulos:\")\n",
    "print(df_ventas.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LimpiadorDatos:\n",
    "    def __init__(self, df):\n",
    "        self.df_original = df.copy()\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def limpiar_fechas(self):\n",
    "        \"\"\"Estandarizar fechas a formato ISO\"\"\"\n",
    "        def parse_fecha(fecha_str):\n",
    "            formatos = ['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%y']\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    return pd.to_datetime(fecha_str, format=fmt)\n",
    "                except:\n",
    "                    continue\n",
    "            return pd.NaT\n",
    "        \n",
    "        self.df['fecha'] = self.df['fecha'].apply(parse_fecha)\n",
    "        return self\n",
    "    \n",
    "    def estandarizar_categorias(self):\n",
    "        \"\"\"Estandarizar nombres de categorías\"\"\"\n",
    "        mapping = {\n",
    "            'electronica': 'Electrónica',\n",
    "            'ELECTRONICA': 'Electrónica',\n",
    "            'Electrnica': 'Electrónica'\n",
    "        }\n",
    "        self.df['categoria'] = self.df['categoria'].replace(mapping)\n",
    "        return self\n",
    "    \n",
    "    def limpiar_precios(self):\n",
    "        \"\"\"Limpiar y validar precios\"\"\"\n",
    "        # Eliminar precios negativos\n",
    "        self.df.loc[self.df['precio'] < 0, 'precio'] = np.nan\n",
    "        \n",
    "        # Detectar y tratar outliers\n",
    "        Q1 = self.df['precio'].quantile(0.25)\n",
    "        Q3 = self.df['precio'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        self.df.loc[self.df['precio'] > Q3 + 1.5*IQR, 'precio'] = Q3 + 1.5*IQR\n",
    "        \n",
    "        # Imputar valores faltantes con la mediana\n",
    "        self.df['precio'] = self.df['precio'].fillna(self.df['precio'].median())\n",
    "        return self\n",
    "    \n",
    "    def validar_emails(self):\n",
    "        \"\"\"Validar formato de emails\"\"\"\n",
    "        patron = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        self.df['email_valido'] = self.df['email'].str.match(patron)\n",
    "        return self\n",
    "    \n",
    "    def eliminar_duplicados(self):\n",
    "        \"\"\"Eliminar registros duplicados\"\"\"\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        return self\n",
    "    \n",
    "    def generar_metricas(self):\n",
    "        \"\"\"Calcular métricas de ventas\"\"\"\n",
    "        self.df['total_venta'] = self.df['precio'] * self.df['cantidad']\n",
    "        return self\n",
    "    \n",
    "    def obtener_reporte_limpieza(self):\n",
    "        \"\"\"Generar reporte de cambios realizados\"\"\"\n",
    "        reporte = {\n",
    "            'registros_originales': len(self.df_original),\n",
    "            'registros_finales': len(self.df),\n",
    "            'duplicados_eliminados': len(self.df_original) - len(self.df),\n",
    "            'emails_invalidos': (~self.df['email_valido']).sum(),\n",
    "            'categorias_unicas': self.df['categoria'].nunique()\n",
    "        }\n",
    "        return reporte\n",
    "\n",
    "# Aplicar limpieza\n",
    "limpiador = LimpiadorDatos(df_ventas)\n",
    "df_limpio = (limpiador\n",
    "    .limpiar_fechas()\n",
    "    .estandarizar_categorias()\n",
    "    .limpiar_precios()\n",
    "    .validar_emails()\n",
    "    .eliminar_duplicados()\n",
    "    .generar_metricas()\n",
    "    .df\n",
    ")\n",
    "\n",
    "# Mostrar reporte\n",
    "reporte = limpiador.obtener_reporte_limpieza()\n",
    "print(\"\\nReporte de Limpieza:\")\n",
    "for k, v in reporte.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Análisis de Datos Limpios')\n",
    "\n",
    "# Ventas por categoría\n",
    "df_limpio.groupby('categoria')['total_venta'].sum().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Ventas por Categoría')\n",
    "\n",
    "# Distribución de precios\n",
    "sns.histplot(data=df_limpio, x='precio', ax=axes[0,1])\n",
    "axes[0,1].set_title('Distribución de Precios')\n",
    "\n",
    "# Ventas por mes\n",
    "df_limpio.set_index('fecha')['total_venta'].resample('M').sum().plot(ax=axes[1,0])\n",
    "axes[1,0].set_title('Ventas Mensuales')\n",
    "\n",
    "# Top productos\n",
    "df_limpio.groupby('producto')['cantidad'].sum().sort_values(ascending=False).plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Productos más Vendidos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Automatización de la Limpieza\n",
    "\n",
    "Vamos a crear una clase para automatizar el proceso de limpieza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AutoLimpiador:\n",
    "    def __init__(self, df):\n",
    "        self.df_original = df.copy()\n",
    "        self.df = df.copy()\n",
    "        self.log = []\n",
    "        \n",
    "    def detectar_tipo_columna(self, columna):\n",
    "        \"\"\"Detectar el tipo de datos de una columna\"\"\"\n",
    "        muestra = self.df[columna].dropna()\n",
    "        if muestra.empty:\n",
    "            return 'unknown'\n",
    "            \n",
    "        # Intentar convertir a datetime\n",
    "        try:\n",
    "            pd.to_datetime(muestra.iloc[0])\n",
    "            return 'fecha'\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Verificar si es numérico\n",
    "        if pd.api.types.is_numeric_dtype(muestra):\n",
    "            return 'numerico'\n",
    "        \n",
    "        # Verificar si es categórico\n",
    "        if muestra.nunique() / len(muestra) < 0.05:\n",
    "            return 'categorico'\n",
    "            \n",
    "        # Verificar si parece un email\n",
    "        if muestra.iloc[0].count('@') == 1:\n",
    "            return 'email'\n",
    "            \n",
    "        return 'texto'\n",
    "    \n",
    "    def limpiar_columna(self, columna):\n",
    "        \"\"\"Aplicar limpieza según el tipo de columna\"\"\"\n",
    "        tipo = self.detectar_tipo_columna(columna)\n",
    "        self.log.append(f\"Limpiando columna {columna} (tipo: {tipo})\")\n",
    "        \n",
    "        if tipo == 'fecha':\n",
    "            self.df[columna] = pd.to_datetime(self.df[columna], errors='coerce')\n",
    "            \n",
    "        elif tipo == 'numerico':\n",
    "            # Convertir a numérico y manejar outliers\n",
    "            self.df[columna] = pd.to_numeric(self.df[columna], errors='coerce')\n",
    "            Q1 = self.df[columna].quantile(0.25)\n",
    "            Q3 = self.df[columna].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            self.df[columna] = self.df[columna].clip(Q1 - 1.5*IQR, Q3 + 1.5*IQR)\n",
    "            \n",
    "        elif tipo == 'categorico':\n",
    "            # Estandarizar categorías\n",
    "            self.df[columna] = self.df[columna].str.strip().str.title()\n",
    "            \n",
    "        elif tipo == 'email':\n",
    "            # Validar emails\n",
    "            patron = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "            mascara = ~self.df[columna].str.match(patron, na=False)\n",
    "            self.df.loc[mascara, columna] = None\n",
    "            \n",
    "        # Imputar valores faltantes\n",
    "        if self.df[columna].isnull().any():\n",
    "            if tipo == 'numerico':\n",
    "                self.df[columna] = self.df[columna].fillna(self.df[columna].median())\n",
    "            elif tipo in ['categorico', 'texto']:\n",
    "                self.df[columna] = self.df[columna].fillna('DESCONOCIDO')\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def limpiar_automaticamente(self):\n",
    "        \"\"\"Limpiar todas las columnas automáticamente\"\"\"\n",
    "        for columna in self.df.columns:\n",
    "            self.limpiar_columna(columna)\n",
    "            \n",
    "        # Eliminar duplicados\n",
    "        n_antes = len(self.df)\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        n_duplicados = n_antes - len(self.df)\n",
    "        self.log.append(f\"Eliminados {n_duplicados} registros duplicados\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def obtener_reporte(self):\n",
    "        \"\"\"Generar reporte de cambios\"\"\"\n",
    "        reporte = {\n",
    "            'registros_originales': len(self.df_original),\n",
    "            'registros_finales': len(self.df),\n",
    "            'valores_nulos_original': self.df_original.isnull().sum().sum(),\n",
    "            'valores_nulos_final': self.df.isnull().sum().sum(),\n",
    "            'log_operaciones': self.log\n",
    "        }\n",
    "        return reporte\n",
    "\n",
    "# Ejemplo de uso\n",
    "auto_limpiador = AutoLimpiador(df_ventas)\n",
    "df_auto_limpio = auto_limpiador.limpiar_automaticamente().df\n",
    "\n",
    "# Mostrar reporte\n",
    "reporte = auto_limpiador.obtener_reporte()\n",
    "print(\"Reporte de Limpieza Automática:\")\n",
    "for k, v in reporte.items():\n",
    "    if k != 'log_operaciones':\n",
    "        print(f\"{k}: {v}\")\n",
    "print(\"\\nLog de operaciones:\")\n",
    "for log in reporte['log_operaciones']:\n",
    "    print(f\"- {log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En esta serie de notebooks hemos cubierto una amplia gama de técnicas de limpieza de datos:\n",
    "\n",
    "1. **Parte 1**: Conceptos básicos y manejo inicial de datos\n",
    "   - Exploración inicial\n",
    "   - Manejo de valores nulos\n",
    "   - Eliminación de duplicados\n",
    "   - Corrección de tipos de datos\n",
    "\n",
    "2. **Parte 2**: Técnicas avanzadas\n",
    "   - Limpieza de fechas con regex\n",
    "   - Estandarización de categorías\n",
    "   - Detección y manejo de outliers\n",
    "   - Validación de datos\n",
    "\n",
    "3. **Parte 3**: Técnicas especializadas y automatización\n",
    "   - Técnicas avanzadas de imputación\n",
    "   - Normalización y escalado\n",
    "   - Caso práctico completo\n",
    "   - Automatización del proceso\n",
    "\n",
    "### Recomendaciones Finales\n",
    "\n",
    "1. Siempre realizar una exploración inicial detallada de los datos\n",
    "2. Documentar todos los pasos de limpieza realizados\n",
    "3. Validar los resultados después de cada transformación\n",
    "4. Mantener una copia de los datos originales\n",
    "5. Automatizar procesos repetitivos de limpieza\n",
    "6. Considerar el impacto de cada transformación en el análisis posterior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
