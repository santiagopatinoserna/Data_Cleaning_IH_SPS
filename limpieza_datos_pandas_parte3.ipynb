{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ч Masterclass de Limpieza de Datos con Pandas - Parte 3\n",
    "\n",
    "## Contenido\n",
    "1. [T茅cnicas Avanzadas de Imputaci贸n](#1)\n",
    "2. [Normalizaci贸n y Escalado](#2)\n",
    "3. [Caso Pr谩ctico: An谩lisis de Ventas](#3)\n",
    "4. [Automatizaci贸n de la Limpieza](#4)\n",
    "\n",
    "En esta tercera parte exploraremos t茅cnicas m谩s sofisticadas de limpieza de datos y veremos un caso pr谩ctico completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuraciones\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. T茅cnicas Avanzadas de Imputaci贸n\n",
    "\n",
    "Vamos a explorar diferentes m茅todos de imputaci贸n para datos faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Crear dataset con valores faltantes\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generar datos correlacionados\n",
    "x = np.random.normal(0, 1, n)\n",
    "y = 2 * x + np.random.normal(0, 0.5, n)\n",
    "z = x - y + np.random.normal(0, 0.3, n)\n",
    "\n",
    "df_missing = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y,\n",
    "    'z': z\n",
    "})\n",
    "\n",
    "# Introducir valores faltantes aleatoriamente\n",
    "for col in df_missing.columns:\n",
    "    mask = np.random.random(n) < 0.2  # 20% de valores faltantes\n",
    "    df_missing.loc[mask, col] = np.nan\n",
    "\n",
    "print(\"Valores faltantes por columna:\")\n",
    "print(df_missing.isnull().sum())\n",
    "\n",
    "# Visualizar patrones de valores faltantes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_missing.isnull(), cmap='viridis', yticklabels=False)\n",
    "plt.title('Patr贸n de Valores Faltantes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Imputaci贸n Simple\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_imputed_mean = pd.DataFrame(\n",
    "    imputer_mean.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "# 2. Imputaci贸n KNN\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df_imputed_knn = pd.DataFrame(\n",
    "    imputer_knn.fit_transform(df_missing),\n",
    "    columns=df_missing.columns\n",
    ")\n",
    "\n",
    "# 3. Imputaci贸n por interpolaci贸n\n",
    "df_imputed_interp = df_missing.interpolate(method='cubic')\n",
    "\n",
    "# Comparar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Comparaci贸n de M茅todos de Imputaci贸n')\n",
    "\n",
    "# Datos originales con valores faltantes\n",
    "sns.scatterplot(data=df_missing, x='x', y='y', ax=axes[0,0])\n",
    "axes[0,0].set_title('Datos Originales con Valores Faltantes')\n",
    "\n",
    "# Imputaci贸n por media\n",
    "sns.scatterplot(data=df_imputed_mean, x='x', y='y', ax=axes[0,1])\n",
    "axes[0,1].set_title('Imputaci贸n por Media')\n",
    "\n",
    "# Imputaci贸n KNN\n",
    "sns.scatterplot(data=df_imputed_knn, x='x', y='y', ax=axes[1,0])\n",
    "axes[1,0].set_title('Imputaci贸n KNN')\n",
    "\n",
    "# Imputaci贸n por interpolaci贸n\n",
    "sns.scatterplot(data=df_imputed_interp, x='x', y='y', ax=axes[1,1])\n",
    "axes[1,1].set_title('Imputaci贸n por Interpolaci贸n')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Normalizaci贸n y Escalado\n",
    "\n",
    "Vamos a explorar diferentes t茅cnicas de normalizaci贸n y escalado de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Crear datos con diferentes escalas\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "df_scale = pd.DataFrame({\n",
    "    'peque帽a_escala': np.random.normal(0, 1, n),\n",
    "    'gran_escala': np.random.normal(1000, 100, n),\n",
    "    'con_outliers': np.concatenate([\n",
    "        np.random.normal(10, 2, n-10),\n",
    "        np.random.normal(50, 5, 10)  # outliers\n",
    "    ])\n",
    "})\n",
    "\n",
    "# Aplicar diferentes escaladores\n",
    "scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'robust': RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_dfs = {}\n",
    "for name, scaler in scalers.items():\n",
    "    scaled_dfs[name] = pd.DataFrame(\n",
    "        scaler.fit_transform(df_scale),\n",
    "        columns=df_scale.columns\n",
    "    )\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Comparaci贸n de M茅todos de Escalado')\n",
    "\n",
    "# Datos originales\n",
    "df_scale.boxplot(ax=axes[0,0])\n",
    "axes[0,0].set_title('Datos Originales')\n",
    "\n",
    "# Standard Scaler\n",
    "scaled_dfs['standard'].boxplot(ax=axes[0,1])\n",
    "axes[0,1].set_title('Standard Scaler')\n",
    "\n",
    "# MinMax Scaler\n",
    "scaled_dfs['minmax'].boxplot(ax=axes[1,0])\n",
    "axes[1,0].set_title('MinMax Scaler')\n",
    "\n",
    "# Robust Scaler\n",
    "scaled_dfs['robust'].boxplot(ax=axes[1,1])\n",
    "axes[1,1].set_title('Robust Scaler')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Caso Pr谩ctico: An谩lisis de Ventas\n",
    "\n",
    "Vamos a crear un caso pr谩ctico completo de limpieza de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Crear dataset de ventas con problemas t铆picos\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generar fechas\n",
    "fechas_base = pd.date_range(start='2022-01-01', end='2023-12-31', periods=n)\n",
    "fechas = [f.strftime(random.choice(['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%y'])) for f in fechas_base]\n",
    "\n",
    "# Generar productos y categor铆as\n",
    "productos = ['Laptop', 'Smartphone', 'Tablet', 'Smartwatch', 'Auriculares']\n",
    "categorias = ['Electr贸nica', 'electronica', 'ELECTRONICA', 'Electrnica']\n",
    "\n",
    "# Crear DataFrame\n",
    "df_ventas = pd.DataFrame({\n",
    "    'fecha': fechas,\n",
    "    'producto': np.random.choice(productos, n),\n",
    "    'categoria': np.random.choice(categorias, n),\n",
    "    'precio': np.random.uniform(100, 1000, n),\n",
    "    'cantidad': np.random.randint(1, 10, n),\n",
    "    'id_cliente': [f'CLI_{i:04d}' for i in range(n)],\n",
    "    'email': [f'cliente{i}@ejemplo.com' if random.random() > 0.1 else 'invalido@.com' for i in range(n)]\n",
    "})\n",
    "\n",
    "# Introducir problemas t铆picos\n",
    "# 1. Valores nulos\n",
    "df_ventas.loc[np.random.choice(n, 50), 'precio'] = None\n",
    "df_ventas.loc[np.random.choice(n, 30), 'email'] = None\n",
    "\n",
    "# 2. Outliers en precios\n",
    "df_ventas.loc[np.random.choice(n, 10), 'precio'] = np.random.uniform(5000, 10000, 10)\n",
    "\n",
    "# 3. Duplicados\n",
    "df_ventas = pd.concat([df_ventas, df_ventas.sample(n=50)])\n",
    "\n",
    "print(\"Estado inicial del dataset:\")\n",
    "print(df_ventas.info())\n",
    "print(\"\\nValores nulos:\")\n",
    "print(df_ventas.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LimpiadorDatos:\n",
    "    def __init__(self, df):\n",
    "        self.df_original = df.copy()\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def limpiar_fechas(self):\n",
    "        \"\"\"Estandarizar fechas a formato ISO\"\"\"\n",
    "        def parse_fecha(fecha_str):\n",
    "            formatos = ['%Y-%m-%d', '%d/%m/%Y', '%d-%m-%y']\n",
    "            for fmt in formatos:\n",
    "                try:\n",
    "                    return pd.to_datetime(fecha_str, format=fmt)\n",
    "                except:\n",
    "                    continue\n",
    "            return pd.NaT\n",
    "        \n",
    "        self.df['fecha'] = self.df['fecha'].apply(parse_fecha)\n",
    "        return self\n",
    "    \n",
    "    def estandarizar_categorias(self):\n",
    "        \"\"\"Estandarizar nombres de categor铆as\"\"\"\n",
    "        mapping = {\n",
    "            'electronica': 'Electr贸nica',\n",
    "            'ELECTRONICA': 'Electr贸nica',\n",
    "            'Electrnica': 'Electr贸nica'\n",
    "        }\n",
    "        self.df['categoria'] = self.df['categoria'].replace(mapping)\n",
    "        return self\n",
    "    \n",
    "    def limpiar_precios(self):\n",
    "        \"\"\"Limpiar y validar precios\"\"\"\n",
    "        # Eliminar precios negativos\n",
    "        self.df.loc[self.df['precio'] < 0, 'precio'] = np.nan\n",
    "        \n",
    "        # Detectar y tratar outliers\n",
    "        Q1 = self.df['precio'].quantile(0.25)\n",
    "        Q3 = self.df['precio'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        self.df.loc[self.df['precio'] > Q3 + 1.5*IQR, 'precio'] = Q3 + 1.5*IQR\n",
    "        \n",
    "        # Imputar valores faltantes con la mediana\n",
    "        self.df['precio'] = self.df['precio'].fillna(self.df['precio'].median())\n",
    "        return self\n",
    "    \n",
    "    def validar_emails(self):\n",
    "        \"\"\"Validar formato de emails\"\"\"\n",
    "        patron = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        self.df['email_valido'] = self.df['email'].str.match(patron)\n",
    "        return self\n",
    "    \n",
    "    def eliminar_duplicados(self):\n",
    "        \"\"\"Eliminar registros duplicados\"\"\"\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        return self\n",
    "    \n",
    "    def generar_metricas(self):\n",
    "        \"\"\"Calcular m茅tricas de ventas\"\"\"\n",
    "        self.df['total_venta'] = self.df['precio'] * self.df['cantidad']\n",
    "        return self\n",
    "    \n",
    "    def obtener_reporte_limpieza(self):\n",
    "        \"\"\"Generar reporte de cambios realizados\"\"\"\n",
    "        reporte = {\n",
    "            'registros_originales': len(self.df_original),\n",
    "            'registros_finales': len(self.df),\n",
    "            'duplicados_eliminados': len(self.df_original) - len(self.df),\n",
    "            'emails_invalidos': (~self.df['email_valido']).sum(),\n",
    "            'categorias_unicas': self.df['categoria'].nunique()\n",
    "        }\n",
    "        return reporte\n",
    "\n",
    "# Aplicar limpieza\n",
    "limpiador = LimpiadorDatos(df_ventas)\n",
    "df_limpio = (limpiador\n",
    "    .limpiar_fechas()\n",
    "    .estandarizar_categorias()\n",
    "    .limpiar_precios()\n",
    "    .validar_emails()\n",
    "    .eliminar_duplicados()\n",
    "    .generar_metricas()\n",
    "    .df\n",
    ")\n",
    "\n",
    "# Mostrar reporte\n",
    "reporte = limpiador.obtener_reporte_limpieza()\n",
    "print(\"\\nReporte de Limpieza:\")\n",
    "for k, v in reporte.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('An谩lisis de Datos Limpios')\n",
    "\n",
    "# Ventas por categor铆a\n",
    "df_limpio.groupby('categoria')['total_venta'].sum().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Ventas por Categor铆a')\n",
    "\n",
    "# Distribuci贸n de precios\n",
    "sns.histplot(data=df_limpio, x='precio', ax=axes[0,1])\n",
    "axes[0,1].set_title('Distribuci贸n de Precios')\n",
    "\n",
    "# Ventas por mes\n",
    "df_limpio.set_index('fecha')['total_venta'].resample('M').sum().plot(ax=axes[1,0])\n",
    "axes[1,0].set_title('Ventas Mensuales')\n",
    "\n",
    "# Top productos\n",
    "df_limpio.groupby('producto')['cantidad'].sum().sort_values(ascending=False).plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('Productos m谩s Vendidos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Automatizaci贸n de la Limpieza\n",
    "\n",
    "Vamos a crear una clase para automatizar el proceso de limpieza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AutoLimpiador:\n",
    "    def __init__(self, df):\n",
    "        self.df_original = df.copy()\n",
    "        self.df = df.copy()\n",
    "        self.log = []\n",
    "        \n",
    "    def detectar_tipo_columna(self, columna):\n",
    "        \"\"\"Detectar el tipo de datos de una columna\"\"\"\n",
    "        muestra = self.df[columna].dropna()\n",
    "        if muestra.empty:\n",
    "            return 'unknown'\n",
    "            \n",
    "        # Intentar convertir a datetime\n",
    "        try:\n",
    "            pd.to_datetime(muestra.iloc[0])\n",
    "            return 'fecha'\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Verificar si es num茅rico\n",
    "        if pd.api.types.is_numeric_dtype(muestra):\n",
    "            return 'numerico'\n",
    "        \n",
    "        # Verificar si es categ贸rico\n",
    "        if muestra.nunique() / len(muestra) < 0.05:\n",
    "            return 'categorico'\n",
    "            \n",
    "        # Verificar si parece un email\n",
    "        if muestra.iloc[0].count('@') == 1:\n",
    "            return 'email'\n",
    "            \n",
    "        return 'texto'\n",
    "    \n",
    "    def limpiar_columna(self, columna):\n",
    "        \"\"\"Aplicar limpieza seg煤n el tipo de columna\"\"\"\n",
    "        tipo = self.detectar_tipo_columna(columna)\n",
    "        self.log.append(f\"Limpiando columna {columna} (tipo: {tipo})\")\n",
    "        \n",
    "        if tipo == 'fecha':\n",
    "            self.df[columna] = pd.to_datetime(self.df[columna], errors='coerce')\n",
    "            \n",
    "        elif tipo == 'numerico':\n",
    "            # Convertir a num茅rico y manejar outliers\n",
    "            self.df[columna] = pd.to_numeric(self.df[columna], errors='coerce')\n",
    "            Q1 = self.df[columna].quantile(0.25)\n",
    "            Q3 = self.df[columna].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            self.df[columna] = self.df[columna].clip(Q1 - 1.5*IQR, Q3 + 1.5*IQR)\n",
    "            \n",
    "        elif tipo == 'categorico':\n",
    "            # Estandarizar categor铆as\n",
    "            self.df[columna] = self.df[columna].str.strip().str.title()\n",
    "            \n",
    "        elif tipo == 'email':\n",
    "            # Validar emails\n",
    "            patron = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "            mascara = ~self.df[columna].str.match(patron, na=False)\n",
    "            self.df.loc[mascara, columna] = None\n",
    "            \n",
    "        # Imputar valores faltantes\n",
    "        if self.df[columna].isnull().any():\n",
    "            if tipo == 'numerico':\n",
    "                self.df[columna] = self.df[columna].fillna(self.df[columna].median())\n",
    "            elif tipo in ['categorico', 'texto']:\n",
    "                self.df[columna] = self.df[columna].fillna('DESCONOCIDO')\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def limpiar_automaticamente(self):\n",
    "        \"\"\"Limpiar todas las columnas autom谩ticamente\"\"\"\n",
    "        for columna in self.df.columns:\n",
    "            self.limpiar_columna(columna)\n",
    "            \n",
    "        # Eliminar duplicados\n",
    "        n_antes = len(self.df)\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        n_duplicados = n_antes - len(self.df)\n",
    "        self.log.append(f\"Eliminados {n_duplicados} registros duplicados\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def obtener_reporte(self):\n",
    "        \"\"\"Generar reporte de cambios\"\"\"\n",
    "        reporte = {\n",
    "            'registros_originales': len(self.df_original),\n",
    "            'registros_finales': len(self.df),\n",
    "            'valores_nulos_original': self.df_original.isnull().sum().sum(),\n",
    "            'valores_nulos_final': self.df.isnull().sum().sum(),\n",
    "            'log_operaciones': self.log\n",
    "        }\n",
    "        return reporte\n",
    "\n",
    "# Ejemplo de uso\n",
    "auto_limpiador = AutoLimpiador(df_ventas)\n",
    "df_auto_limpio = auto_limpiador.limpiar_automaticamente().df\n",
    "\n",
    "# Mostrar reporte\n",
    "reporte = auto_limpiador.obtener_reporte()\n",
    "print(\"Reporte de Limpieza Autom谩tica:\")\n",
    "for k, v in reporte.items():\n",
    "    if k != 'log_operaciones':\n",
    "        print(f\"{k}: {v}\")\n",
    "print(\"\\nLog de operaciones:\")\n",
    "for log in reporte['log_operaciones']:\n",
    "    print(f\"- {log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En esta serie de notebooks hemos cubierto una amplia gama de t茅cnicas de limpieza de datos:\n",
    "\n",
    "1. **Parte 1**: Conceptos b谩sicos y manejo inicial de datos\n",
    "   - Exploraci贸n inicial\n",
    "   - Manejo de valores nulos\n",
    "   - Eliminaci贸n de duplicados\n",
    "   - Correcci贸n de tipos de datos\n",
    "\n",
    "2. **Parte 2**: T茅cnicas avanzadas\n",
    "   - Limpieza de fechas con regex\n",
    "   - Estandarizaci贸n de categor铆as\n",
    "   - Detecci贸n y manejo de outliers\n",
    "   - Validaci贸n de datos\n",
    "\n",
    "3. **Parte 3**: T茅cnicas especializadas y automatizaci贸n\n",
    "   - T茅cnicas avanzadas de imputaci贸n\n",
    "   - Normalizaci贸n y escalado\n",
    "   - Caso pr谩ctico completo\n",
    "   - Automatizaci贸n del proceso\n",
    "\n",
    "### Recomendaciones Finales\n",
    "\n",
    "1. Siempre realizar una exploraci贸n inicial detallada de los datos\n",
    "2. Documentar todos los pasos de limpieza realizados\n",
    "3. Validar los resultados despu茅s de cada transformaci贸n\n",
    "4. Mantener una copia de los datos originales\n",
    "5. Automatizar procesos repetitivos de limpieza\n",
    "6. Considerar el impacto de cada transformaci贸n en el an谩lisis posterior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
